{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset retrieved from [Kaggle](https://www.kaggle.com/henriqueyamahata/bank-marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is filled with outcomes of marketing campaign phone calls from a Portuguese banking institution. Each phone call has a target to get the customer to subscribe to a term deposit at a bank institution. This is what we will be training the model to predict, so this is a binary classification problem.\n",
    "\n",
    "Having a trained model to be able to predict whether a customer will subscribe or not can be really beneficial for sales representatives. They can prioritize individuals who are more likely to subscribe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some attributes of the dataset that we will have to handle are that there is a lot of categorical features that will need to be encoded along with an imbalance between the two binary classifiers across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model Helpers\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is well prepared with no null values. The biggest thing that needs to be tackled is encoding of categorical features and handling the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bank-marketing.csv')\n",
    "pd.options.display.max_columns = None\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the age distribution of the callers. Visualization code retrieved from [here](https://www.kaggle.com/henriqueyamahata/bank-marketing-classification-roc-f1-recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = data.sort_values('age')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18, 7)\n",
    "sns.countplot(x = 'age', data = data.sort_values('age'))\n",
    "ax.set_xlabel('Age', fontsize=15)\n",
    "ax.set_ylabel('Count', fontsize=15)\n",
    "ax.set_title('Age Count Distribution', fontsize=15)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the distribution of marital status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "sns.countplot(x = 'marital', data = data, order=data['marital'].value_counts().index)\n",
    "ax.set_xlabel('Marital Status', fontsize=15)\n",
    "ax.set_ylabel('Count', fontsize=15)\n",
    "ax.set_title('Marital Status Distribution', fontsize=15)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the distribution of education levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 6)\n",
    "sns.countplot(x = 'education', data = data.sort_values('education'), order=data['education'].value_counts().index)\n",
    "ax.set_xlabel('Education Level', fontsize=15)\n",
    "ax.set_ylabel('Count', fontsize=15)\n",
    "ax.set_title('Education Distribution', fontsize=15)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 5)\n",
    "sns.countplot(x = 'job', data = data, order=data['job'].value_counts().index)\n",
    "ax.set_xlabel('Job', fontsize=15)\n",
    "ax.set_ylabel('Count', fontsize=15)\n",
    "ax.set_title('Job Type Distribution', fontsize=15)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Subscribed Customers on Each Day of The Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main takeaways I noticed from the chart below is it seems that people are much less likely to subscribe to bank term deposits on Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "\n",
    "plt.bar(y_class, [(np.sum((data['y'] == 'yes') & (data['day_of_week'] == 'mon')) \n",
    "                   / np.sum(data['day_of_week'] == 'mon')) * 100, \n",
    "                  (np.sum((data['y'] == 'yes') & (data['day_of_week'] == 'tue')) \n",
    "                   / np.sum(data['day_of_week'] == 'tue')) * 100,\n",
    "                  (np.sum((data['y'] == 'yes') & (data['day_of_week'] == 'wed')) \n",
    "                   / np.sum(data['day_of_week'] == 'wed')) * 100,\n",
    "                  (np.sum((data['y'] == 'yes') & (data['day_of_week'] == 'thu')) \n",
    "                   / np.sum(data['day_of_week'] == 'thu')) * 100,\n",
    "                  (np.sum((data['y'] == 'yes') & (data['day_of_week'] == 'fri')) \n",
    "                   / np.sum(data['day_of_week'] == 'fri')) * 100],\n",
    "                color=['brown', 'red', 'purple', 'orange', 'teal'])\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Subscribed Customer Percentage\")\n",
    "plt.title(\"Percentage of Customers Called Who Subscribed Per Day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Subscribed and Non Subscribed Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can visually see, the dataset is very skewed. If the data is run through a model without being stratified, it will cause us to get improper accuracy results, as about 90% of client phone calls in the dataset did not subscribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = ['Didn\\'t Subscribe', 'Subscribed']\n",
    "\n",
    "plt.bar(y_class, [np.sum(data['y'] == 'no'), np.sum(data['y'] == 'yes')], color=['teal', 'orange'])\n",
    "plt.xlabel(\"Outcome\")\n",
    "plt.ylabel(\"Phone Calls\")\n",
    "plt.title(\"Distribution of Customers Who Did and Didn't Subscribe\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will bin certain categorical features to try and simplify the dataset for better learning. The age and duration function code snippets I retrieved from [here](https://www.kaggle.com/henriqueyamahata/bank-marketing-imbalanced-dataset-94)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_education = {\n",
    "    \"university.degree\": \"university.degree\",\n",
    "    \"professional.course\": \"professional.course\",\n",
    "    \"high.school\": \"high.school\",\n",
    "    \"basic.9y\": \"basic\",\n",
    "    \"basic.6y\": \"basic\",\n",
    "    \"basic.4y\": \"basic\",\n",
    "    \"unknown\": \"unknown\",\n",
    "    \"illiterate\": \"unknown\"\n",
    "}\n",
    "data.education = data.education.map(bin_education)\n",
    "\n",
    "def age(dataframe):\n",
    "    dataframe.loc[dataframe['age'] <= 32, 'age'] = 1\n",
    "    dataframe.loc[(dataframe['age'] > 32) & (dataframe['age'] <= 47), 'age'] = 2\n",
    "    dataframe.loc[(dataframe['age'] > 47) & (dataframe['age'] <= 70), 'age'] = 3\n",
    "    dataframe.loc[(dataframe['age'] > 70) & (dataframe['age'] <= 98), 'age'] = 4\n",
    "           \n",
    "    return dataframe\n",
    "\n",
    "data = age(data)\n",
    "\n",
    "def duration(data):\n",
    "\n",
    "    data.loc[data['duration'] <= 102, 'duration'] = 1\n",
    "    data.loc[(data['duration'] > 102) & (data['duration'] <= 180)  , 'duration']    = 2\n",
    "    data.loc[(data['duration'] > 180) & (data['duration'] <= 319)  , 'duration']   = 3\n",
    "    data.loc[(data['duration'] > 319) & (data['duration'] <= 644.5), 'duration'] = 4\n",
    "    data.loc[data['duration']  > 644.5, 'duration'] = 5\n",
    "\n",
    "    return data\n",
    "\n",
    "data = duration(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am applying a form of one hot encoding to categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['job'], prefix = ['job'])\n",
    "data = pd.get_dummies(data, columns=['marital'], prefix = ['marital'])\n",
    "data = pd.get_dummies(data, columns=['education'], prefix = ['education'])\n",
    "data = pd.get_dummies(data, columns=['default'], prefix = ['default'])\n",
    "data = pd.get_dummies(data, columns=['housing'], prefix = ['housing'])\n",
    "data = pd.get_dummies(data, columns=['loan'], prefix = ['loan'])\n",
    "data = pd.get_dummies(data, columns=['contact'], prefix = ['contact'])\n",
    "data = pd.get_dummies(data, columns=['month'], prefix = ['month'])\n",
    "data = pd.get_dummies(data, columns=['day_of_week'], prefix = ['day_of_week'])\n",
    "data = pd.get_dummies(data, columns=['poutcome'], prefix = ['poutcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of days since last call feature has a value of 999 if the customer hasn't been called before. This can possibly create unwanted bias. Instead of setting a wild number that has an unknown purpose, 999 should be set to 0 and a new featured named 'called_before' should be created that will be a boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['called_before'] = data.pdays.apply(lambda row: 0 if (row == 999) else 1)\n",
    "data['pdays'] = data.pdays.apply(lambda row: 0 if (row == 999) else row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target column is currently either 'yes' or 'no' strings. That can't be fed into the model, so it should be encoded to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'] = data.y.apply(lambda row: 1 if (row == 'yes') else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 4,640 samples out of 41,188 samples where clients have subscribed to bank term deposits. I will use a form of resampling to oversample the clients who have subscribed which will in return double our amount of subscribed samples. To properly train the model, the training set and test set will both be stratified in a 80/20 split. This means that 80% of the non-subscribed and subscribed samples will be used for training and 20% will be used for testing. That allows for the model to properly learn from the dataset and allow us to see true performance metrics, as the two sets will be properly stratified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide up the data based on its predetermined label and assign to variables for easy management and also randomize each labeled variable to give the model a variety of inputs. I decided to double the subscribed section of the dataset when resampling, as I didn't want to overfit the models with too many synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_not_sub, X_sub = data[data.y == 0], data[data.y == 1]\n",
    "\n",
    "# Resample the subscribed samples\n",
    "X_sub = resample(X_sub, replace=True, n_samples=int(4640*2), random_state=123) \n",
    "y_not_sub, y_sub = X_not_sub.pop('y'), X_sub.pop('y')\n",
    "\n",
    "X = X_not_sub.append(X_sub)\n",
    "y = y_not_sub.append(y_sub)\n",
    "\n",
    "print(\"Not subscribed: %s \\t Subscribed: %s\" % (X_not_sub.shape[0], X_sub.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets for Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest doesn't require scaled data and isn't going to use PCA. Logistic Regression requires data to be scaled, and we will be using PCA as well to see if there are any benefits. Initially, I wasn't shuffling the two sets, so I was getting bad performance measures when testing models. This was happening because the test data was completely different to what the model had trained on, so once I added the shuffling, it cleared up and performed much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# Logistic Regression Sets\n",
    "sc = StandardScaler() #center the distribution around zero (mean), with a standard deviation of 1.\n",
    "sc.fit(X)\n",
    "X_std = sc.transform(X)\n",
    "\n",
    "X_pca = PCA(0.99, svd_solver='full').fit_transform(X_std)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_std, y, test_size=0.20, random_state=42, shuffle=True, stratify=y)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will be using the Random Forest model accompanied with GridSearchCV to help find the optimal hyper-parameters when fitting the model. We then will test the model's performance using accuracy, precision, recall, the AUC derived from the ROC curve, the sum of samples that were misclassified, and also an overall classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(random_state=42, n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the GridSearchCV parameters and fit the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_params = [\n",
    "   { \n",
    "     'max_depth': [None, 2, 4, 6, 8, 10, 12], \n",
    "     'min_samples_leaf' : [1, 2, 3, 4, 5],\n",
    "     'max_leaf_nodes': [None, 4, 8, 12, 14],\n",
    "   },\n",
    "]\n",
    "\n",
    "rnd_cv = GridSearchCV(estimator=rnd_clf, param_grid=rnd_params, cv=4)\n",
    "rnd_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal params: {}'.format(rnd_cv.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below retrieved from [here](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html). This is showing the feature importance after the model has been trained. It showed in descending order which features had the most impact on predicting. This was very insightful to see, as I originally was using it to drop specific features that I didn't need. However, I found that when I dropped unimportant features, the learning performance measure would go down. For that reason and the fact that there isn't a huge amount of features, I opted to not drop any features during the data engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rnd_cv.best_estimator_.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rnd_cv.best_estimator_.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "\n",
    "#for name, importance in zip(X_train, rnd_cv.best_estimator_.feature_importances_):\n",
    "#    print(name, \"=\", importance)\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"Feature: %s, Index: %d (%f)\" % (X_train.columns[f], indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Top 10 Features Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(10), indices)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cross validation score on the training data prior to testing on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(rnd_cv, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy, precision, and recall on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_cv.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score: \", (accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision score: \", (precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \", (recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('Area under the curve: {}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Misclassified samples: ' + str((y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Report\\n',classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try the Logistic Regression model using GridSearchCV to automate hyper-parameters. We then will test the model's performance using accuracy, precision, recall, the AUC derived from the ROC curve, the sum of samples that were misclassified, and also an overall classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(solver=\"lbfgs\", random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cross validation score on the training data prior to testing on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the GridSearchCV parameters and fit the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = [\n",
    "   { \n",
    "        'max_iter': [500, 1000, 1500],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "   },\n",
    "]\n",
    "\n",
    "lr_cv = GridSearchCV(estimator=lr_clf, param_grid=lr_params, cv=4)\n",
    "lr_cv.fit(X_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal params: {}'.format(lr_cv.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cross validation score on the training data prior to testing on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(lr_cv.best_estimator_, X_train_pca, y_train_pca, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy, precision, and recall on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pca = lr_cv.predict(X_test_pca)\n",
    "\n",
    "print(\"Accuracy score: \", (accuracy_score(y_test_pca, y_pred_pca)))\n",
    "print(\"Precision score: \", (precision_score(y_test_pca, y_pred_pca)))\n",
    "print(\"Recall score: \", (recall_score(y_test_pca, y_pred_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_pca, y_pred_pca)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('Area under the curve: {}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Misclassified samples: ' + str((y_test_pca != y_pred_pca).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression Report\\n',classification_report(y_test_pca, y_pred_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest yields better results than Logistic Regression on all performance calculations.\n",
    "\n",
    "When dealing with imbalanced datasets, you might find that there aren't enough of one labeled category to aid in learning. Resampling via oversampling or undersampling using different techniques such as Synthetic Minority Oversampling Technique (SMOTE) might be able to help. In my case, it help a lot overall to train the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
